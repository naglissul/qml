# Theory in chronological learning order ARCHIVED

## 2024-07-03

Started from this tutorial (+ chatgpt ofc):
[https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/](https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/)

Python script here: [scripts/neural1.py](scripts/neural1.py)

Stopped at the forward propagation, cuz seemed too much of code to understand the whole idea.

Now gonna learn separated parts of this. Starting with Gradient descent and Stochastic gradient descent.

## 2024-07-06

### Gradient descent

Basically finding local minimum by going the direction of negative derivative (gradient)

### In general

AI > ML > DL

Supervised and unsupervised learning.

## 2024-07-11

Linear classification, perceptron neuron (linear classifier), only feed-forward...

### Learning XOR - non-linear classification with backpropagation.

Input Layer | Hidden Layer | Output Layer\\
(2 neurons) | (2 neurons) | (1 neuron)

[scripts/xor/](scripts/xor/)

## 2024-07-14

Finishing XOR and understanding it...

Already has a simulator...
https://www.mladdict.com/neural-network-simulator

## 2024-07-15

[Building XOR on my own](scripts/myXor/)
